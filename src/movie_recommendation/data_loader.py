"""
Data loading and preprocessing module.
"""

import pandas as pd
import numpy as np
import kagglehub
from typing import Tuple, Optional
import logging
import os

logger = logging.getLogger(__name__)


class DataLoader:
    """Load and preprocess MovieLens dataset."""
    
    def __init__(self, dataset_name: str = "grouplens/movielens-20m-dataset"):
        """
        Initialize DataLoader.
        
        Args:
            dataset_name: Name of the dataset in kagglehub
        """
        self.dataset_name = dataset_name
        self.path = None
        self.movies = None
        self.ratings = None
        
    def load_data(self, limit: Optional[int] = None, min_item_ratings: int = 0) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        Load MovieLens dataset.
        
        Args:
            limit: Optional limit on number of ratings to load
            min_item_ratings: Minimum number of ratings per item (0 = no filtering)
            
        Returns:
            Tuple of (movies, ratings) DataFrames
        """
        logger.info(f"è¼‰å…¥è³‡æ–™é›†: {self.dataset_name}")
        self.path = kagglehub.dataset_download(self.dataset_name)
        
        self.movies = pd.read_csv(f"{self.path}/movie.csv")[["movieId", "title"]]
        
        if limit:
            ratings_full = pd.read_csv(f"{self.path}/rating.csv")
            self.ratings = ratings_full[["userId", "movieId", "rating"]].iloc[:limit]
        else:
            self.ratings = pd.read_csv(f"{self.path}/rating.csv")[["userId", "movieId", "rating"]]
        
        # éæ¿¾é•·å°¾é›»å½±
        if min_item_ratings > 0:
            self.ratings = self.filter_cold_items(self.ratings, min_item_ratings)
        
        logger.info(f"è¼‰å…¥å®Œæˆ: {len(self.movies)} éƒ¨é›»å½±, {len(self.ratings)} ç­†è©•åˆ†")
        return self.movies, self.ratings
    
    def load_with_timestamp(self, limit: Optional[int] = None) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        Load MovieLens dataset with timestamp for time-based features.
        
        Args:
            limit: Optional limit on number of ratings to load
            
        Returns:
            Tuple of (movies, ratings) DataFrames with timestamp
        """
        logger.info(f"è¼‰å…¥è³‡æ–™é›† (å«æ™‚é–“æˆ³è¨˜): {self.dataset_name}")
        self.path = kagglehub.dataset_download(self.dataset_name)
        
        self.movies = pd.read_csv(f"{self.path}/movie.csv")[["movieId", "title"]]
        
        if limit:
            ratings_full = pd.read_csv(
                f"{self.path}/rating.csv",
                usecols=["userId", "movieId", "rating", "timestamp"],
                dtype={'userId': int, 'movieId': int, 'rating': float, 'timestamp': object}
            )
            self.ratings = ratings_full.iloc[:limit]
        else:
            self.ratings = pd.read_csv(
                f"{self.path}/rating.csv",
                usecols=["userId", "movieId", "rating", "timestamp"],
                dtype={'userId': int, 'movieId': int, 'rating': float, 'timestamp': object}
            )
        
        # Parse timestamp
        self.ratings['timestamp'] = pd.to_datetime(self.ratings['timestamp'], errors='coerce')
        self.ratings = self.ratings.dropna(subset=['timestamp'])
        
        logger.info(f"è¼‰å…¥å®Œæˆ: {len(self.movies)} éƒ¨é›»å½±, {len(self.ratings)} ç­†æœ‰æ•ˆè©•åˆ†")
        return self.movies, self.ratings
    
    def create_user_item_mapping(self, ratings: pd.DataFrame) -> Tuple[pd.DataFrame, dict, dict]:
        """
        Create user and item index mappings.
        
        Args:
            ratings: Ratings DataFrame
            
        Returns:
            Tuple of (ratings with indices, user_map, movie_map)
        """
        ratings = ratings.copy()
        ratings['user_idx'] = ratings['userId'].astype('category').cat.codes
        ratings['movie_idx'] = ratings['movieId'].astype('category').cat.codes
        
        user_map = dict(enumerate(ratings['userId'].astype('category').cat.categories))
        movie_map = dict(enumerate(ratings['movieId'].astype('category').cat.categories))
        
        n_users = len(user_map)
        n_items = len(movie_map)
        
        logger.info(f"å»ºç«‹æ˜ å°„: {n_users} ä½ä½¿ç”¨è€…, {n_items} éƒ¨é›»å½±")
        return ratings, user_map, movie_map
    
    def calculate_item_bias(self, ratings: pd.DataFrame) -> Tuple[dict, float]:
        """
        Calculate item mean ratings (item bias).
        
        Args:
            ratings: Ratings DataFrame with movie_idx
            
        Returns:
            Tuple of (item_means dict, global_mean)
        """
        item_means = ratings.groupby('movie_idx')['rating'].mean().to_dict()
        global_mean = ratings['rating'].mean()
        
        logger.info(f"è¨ˆç®— Item Bias: å…¨å±€å¹³å‡ = {global_mean:.4f}")
        return item_means, global_mean
    
    def filter_cold_items(
        self,
        ratings: pd.DataFrame,
        min_item_ratings: int = 10
    ) -> pd.DataFrame:
        """
        éæ¿¾è©•åˆ†æ•¸ä¸è¶³çš„é›»å½±ï¼ˆé•·å°¾é›»å½±ï¼‰
        
        Args:
            ratings: è©•åˆ†è³‡æ–™
            min_item_ratings: é›»å½±æœ€å°‘è©•åˆ†æ•¸é–¾å€¼
            
        Returns:
            éæ¿¾å¾Œçš„è©•åˆ†è³‡æ–™
        """
        if min_item_ratings <= 0:
            return ratings
        
        item_counts = ratings.groupby('movieId').size()
        valid_items = item_counts[item_counts >= min_item_ratings].index
        filtered_ratings = ratings[ratings['movieId'].isin(valid_items)]
        
        removed_items = len(item_counts) - len(valid_items)
        removed_ratings = len(ratings) - len(filtered_ratings)
        
        logger.info(
            f"é›»å½±éæ¿¾: ç§»é™¤ {removed_items} éƒ¨é›»å½± "
            f"({removed_items/len(item_counts)*100:.2f}%), "
            f"{removed_ratings:,} ç­†è©•åˆ† "
            f"({removed_ratings/len(ratings)*100:.2f}%)"
        )
        
        return filtered_ratings

    def get_genome_path(self) -> Optional[str]:
        """
        å˜—è©¦å°‹æ‰¾ä¸¦å›å‚³ genome-scores.csv çš„çµ•å°è·¯å¾‘
        æ”¯æ´éè¿´æœå°‹ã€ä¸åŒå‘½åæ ¼å¼ (åº•ç·š/é€£å­—è™Ÿ) èˆ‡å¤§å°å¯«å¿½ç•¥
        """
        # 1. ç¢ºä¿è³‡æ–™å·²ä¸‹è¼‰
        if self.path is None:
            logger.info("æª¢æŸ¥åŸºå› è³‡æ–™ä¸­ï¼Œæ­£åœ¨ç¢ºèªè³‡æ–™é›†ä¸‹è¼‰ç‹€æ…‹...")
            self.path = kagglehub.dataset_download(self.dataset_name)
            
        logger.info(f"æ­£åœ¨æœå°‹åŸºå› æª”æ¡ˆï¼Œæœå°‹æ ¹ç›®éŒ„: {self.path}")
        
        # 2. å®šç¾©ç›®æ¨™æª”å (è½‰æˆå°å¯«ä»¥æ–¹ä¾¿æ¯”å°)
        # Kaggle è³‡æ–™é›†æœ‰æ™‚å€™æœƒæ”¹æª”åï¼Œä¾‹å¦‚æŠŠé€£å­—è™Ÿæ”¹æˆåº•ç·š
        target_names = {'genome-scores.csv', 'genome_scores.csv', 'genome_scores.csv.zip'}
        
        # 3. ä½¿ç”¨ os.walk é€²è¡Œåœ°æ¯¯å¼æœç´¢
        for root, dirs, files in os.walk(self.path):
            for filename in files:
                # è½‰å°å¯«æ¯”å°ï¼Œå¢åŠ å®¹éŒ¯ç‡
                if filename.lower() in target_names:
                    full_path = os.path.join(root, filename)
                    logger.info(f"âœ… æˆåŠŸå®šä½åŸºå› æª”æ¡ˆ: {full_path}")
                    return full_path
        
        # 4. å¦‚æœè·‘åˆ°é€™è£¡ä»£è¡¨çœŸçš„æ²’æ‰¾åˆ°ï¼Œå°å‡ºç›®éŒ„çµæ§‹å¹«åŠ©é™¤éŒ¯
        logger.error("âŒ æ‰¾ä¸åˆ°ç›®æ¨™æª”æ¡ˆã€‚ä»¥ä¸‹æ˜¯æœå°‹éçš„ç›®éŒ„çµæ§‹æ‘˜è¦:")
        try:
            # åªå°å‡ºå‰ 3 å±¤ç›®éŒ„çµæ§‹ï¼Œé¿å… Log çˆ†ç‚¸
            level_limit = 3
            root_depth = self.path.count(os.sep)
            
            for root, dirs, files in os.walk(self.path):
                current_depth = root.count(os.sep)
                if current_depth - root_depth < level_limit:
                    indent = "  " * (current_depth - root_depth)
                    logger.error(f"{indent}ğŸ“‚ {os.path.basename(root)}/")
                    for f in files[:5]: # æ¯å€‹ç›®éŒ„åªåˆ—å‡ºå‰ 5 å€‹æª”æ¡ˆ
                        logger.error(f"{indent}  ğŸ“„ {f}")
                    if len(files) > 5:
                        logger.error(f"{indent}  ... (é‚„æœ‰ {len(files)-5} å€‹æª”æ¡ˆ)")
        except Exception as e:
            logger.error(f"ç„¡æ³•åˆ—å°ç›®éŒ„çµæ§‹: {e}")
            
        return None

