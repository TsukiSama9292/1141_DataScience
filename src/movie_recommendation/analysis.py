"""
å¯¦é©—çµæœåˆ†ææ¨¡çµ„

æä¾›å„éšæ®µå¯¦é©—çµæœçš„åˆ†æåŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
- é€²åº¦æª¢æŸ¥
- SVD ç¶­åº¦åˆ†æ
- KNN Kå€¼åˆ†æ
- æœ€ä½³é…ç½®æå–
- è³‡æ–™é›†çµ±è¨ˆåˆ†æ
"""

import json
import re
import pandas as pd
import numpy as np
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
from collections import defaultdict


class ExperimentAnalyzer:
    """å¯¦é©—çµæœåˆ†æå™¨"""
    
    def __init__(self, log_dir: str = 'log', run_dir: str = 'run', data_dir: str = 'data'):
        self.log_dir = Path(log_dir)
        self.run_dir = Path(run_dir)
        self.data_dir = Path(data_dir)
        
        # å„éšæ®µé…ç½®å®šç¾©
        self.stages = {
            'DS': ['DS_001', 'DS_002', 'DS_003', 'DS_004'],
            'FILTER': ['FILTER_001', 'FILTER_002', 'FILTER_003', 
                      'FILTER_004', 'FILTER_005', 'FILTER_006'],
            'KNN_BASELINE': [f'KNN_BASELINE_{i:03d}' for i in range(1, 11)],
            'SVD': [f'SVD_{i:03d}' for i in range(1, 16)],
            'KNN': [f'KNN_{i:03d}' for i in range(1, 8)],
            'SVD_KNN_EXPAND': [f'SVD_KNN_EXPAND_{i:03d}' for i in range(1, 37)],
            'BIAS': ['BIAS_001', 'BIAS_002'],
            'OPT': ['OPT_001', 'OPT_002']
        }
    
    def load_result(self, config_name: str) -> Optional[Dict[str, Any]]:
        """è¼‰å…¥å–®å€‹é…ç½®çš„çµæœ"""
        json_file = self.log_dir / f"{config_name}.json"
        if not json_file.exists():
            return None
        
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"âš ï¸ ç„¡æ³•è®€å– {json_file}: {e}")
            return None
    
    def load_stage_results(self, stage: str) -> List[Dict[str, Any]]:
        """è¼‰å…¥æŒ‡å®šéšæ®µçš„æ‰€æœ‰çµæœ"""
        results = []
        for config_name in self.stages.get(stage, []):
            data = self.load_result(config_name)
            if data:
                results.append({
                    'config_name': config_name,
                    'data': data
                })
        return results
    
    def extract_param_from_config(self, config_name: str, param_pattern: str) -> Optional[int]:
        """å¾é…ç½®æ–‡ä»¶ä¸­æå–åƒæ•¸å€¼"""
        config_file = self.run_dir / f"{config_name}.py"
        if not config_file.exists():
            return None
        
        try:
            content = config_file.read_text(encoding='utf-8')
            match = re.search(param_pattern, content)
            if match:
                return int(match.group(1))
        except Exception:
            pass
        return None
    
    def check_progress(self) -> Dict[str, Any]:
        """æª¢æŸ¥åŸ·è¡Œé€²åº¦"""
        if not self.log_dir.exists():
            return {
                'total_completed': 0,
                'total_configs': sum(len(configs) for configs in self.stages.values()),
                'stages': {},
                'analysis_status': {'svd': False, 'knn': False}
            }
        
        stage_progress = {}
        total_completed = 0
        
        for stage_name, configs in self.stages.items():
            completed = []
            missing = []
            
            for config in configs:
                json_file = self.log_dir / f"{config}.json"
                if json_file.exists():
                    completed.append(config)
                else:
                    missing.append(config)
            
            stage_progress[stage_name] = {
                'completed': completed,
                'missing': missing,
                'total': len(configs),
                'count': len(completed),
                'rate': len(completed) / len(configs) * 100
            }
            total_completed += len(completed)
        
        # æª¢æŸ¥åˆ†æç‹€æ…‹
        best_svd = (self.log_dir / 'best_svd.json').exists()
        best_knn = (self.log_dir / 'best_knn.json').exists()
        
        return {
            'total_completed': total_completed,
            'total_configs': sum(len(configs) for configs in self.stages.values()),
            'stages': stage_progress,
            'analysis_status': {
                'svd': best_svd,
                'knn': best_knn
            }
        }
    
    def analyze_svd(self) -> Optional[Dict[str, Any]]:
        """åˆ†æ SVD éšæ®µçµæœ"""
        results = self.load_stage_results('SVD')
        
        if not results:
            return None
        
        # æå–ä¸¦æ’åºçµæœ
        analysis = []
        for result in results:
            config_name = result['config_name']
            data = result['data']
            
            # æå– SVD ç¶­åº¦
            n_components = self.extract_param_from_config(
                config_name, r'n_components\s*=\s*(\d+)'
            )
            
            if n_components:
                metrics = data.get('metrics', {})
                time_records = data.get('time_records', {})
                
                analysis.append({
                    'config_name': config_name,
                    'n_components': n_components,
                    'hit_rate': metrics.get('hit_rate', 0),
                    'ndcg': metrics.get('ndcg', 0),
                    'rmse': metrics.get('rmse', 0),
                    'total_time': sum(time_records.values()) if time_records else 0,
                    'memory_mb': data.get('peak_memory_mb', 0)
                })
        
        # æŒ‰ Hit Rate æ’åº
        analysis.sort(key=lambda x: x['hit_rate'], reverse=True)
        
        if analysis:
            best = analysis[0]
            # ä¿å­˜æœ€ä½³é…ç½®
            best_file = self.log_dir / 'best_svd.json'
            with open(best_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'best_config': best['config_name'],
                    'best_n_components': best['n_components'],
                    'metrics': {
                        'hit_rate': best['hit_rate'],
                        'ndcg': best['ndcg'],
                        'rmse': best['rmse']
                    }
                }, f, indent=2, ensure_ascii=False)
        
        return {
            'results': analysis,
            'best': analysis[0] if analysis else None,
            'count': len(analysis)
        }
    
    def analyze_knn(self) -> Optional[Dict[str, Any]]:
        """åˆ†æ KNN éšæ®µçµæœ"""
        results = self.load_stage_results('KNN')
        
        if not results:
            return None
        
        # æå–ä¸¦æ’åºçµæœ
        analysis = []
        for result in results:
            config_name = result['config_name']
            data = result['data']
            
            # æå– K å€¼
            k_neighbors = self.extract_param_from_config(
                config_name, r'k_neighbors\s*=\s*(\d+)'
            )
            
            if k_neighbors:
                metrics = data.get('metrics', {})
                time_records = data.get('time_records', {})
                
                analysis.append({
                    'config_name': config_name,
                    'k_neighbors': k_neighbors,
                    'hit_rate': metrics.get('hit_rate', 0),
                    'ndcg': metrics.get('ndcg', 0),
                    'rmse': metrics.get('rmse', 0),
                    'total_time': sum(time_records.values()) if time_records else 0
                })
        
        # æŒ‰ Hit Rate æ’åº
        analysis.sort(key=lambda x: x['hit_rate'], reverse=True)
        
        if analysis:
            best = analysis[0]
            # ä¿å­˜æœ€ä½³é…ç½®
            best_file = self.log_dir / 'best_knn.json'
            with open(best_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'best_config': best['config_name'],
                    'best_k_neighbors': best['k_neighbors'],
                    'metrics': {
                        'hit_rate': best['hit_rate'],
                        'ndcg': best['ndcg'],
                        'rmse': best['rmse']
                    }
                }, f, indent=2, ensure_ascii=False)
        
        return {
            'results': analysis,
            'best': analysis[0] if analysis else None,
            'count': len(analysis)
        }
    
    def get_best_configs(self) -> Dict[str, Any]:
        """ç²å–æ‰€æœ‰æœ€ä½³é…ç½®"""
        best_configs = {}
        
        # è®€å–ä¿å­˜çš„æœ€ä½³é…ç½®
        best_svd_file = self.log_dir / 'best_svd.json'
        if best_svd_file.exists():
            with open(best_svd_file, 'r', encoding='utf-8') as f:
                best_configs['svd'] = json.load(f)
        
        best_knn_file = self.log_dir / 'best_knn.json'
        if best_knn_file.exists():
            with open(best_knn_file, 'r', encoding='utf-8') as f:
                best_configs['knn'] = json.load(f)
        
        return best_configs
    
    def generate_summary_table(self) -> List[Dict[str, Any]]:
        """ç”Ÿæˆæ‰€æœ‰éšæ®µçš„æ‘˜è¦è¡¨æ ¼"""
        summary = []
        
        # ç§»é™¤ DS éšæ®µï¼Œå› ç‚ºå®ƒæ¸¬è©¦çš„æ˜¯è³‡æ–™é‡è€Œéè¶…åƒæ•¸
        for stage_name in ['FILTER', 'KNN_BASELINE', 'SVD_KNN_GRID', 'SVD_KNN_EXPAND', 'BIAS', 'OPT']:
            # SVD_KNN_GRID å’Œ SVD_KNN_EXPAND ç‰¹æ®Šè™•ç†
            if stage_name in ['SVD_KNN_GRID', 'SVD_KNN_EXPAND']:
                # è¼‰å…¥æ‰€æœ‰çµæœ
                log_dir = Path(self.log_dir)
                grid_results = []
                max_exp = 101 if stage_name == 'SVD_KNN_GRID' else 37
                for i in range(1, max_exp):
                    config_name = f'{stage_name}_{i:03d}'
                    json_file = log_dir / f'{config_name}.json'
                    if json_file.exists():
                        try:
                            with open(json_file, 'r') as f:
                                data = json.load(f)
                            grid_results.append({'config_name': config_name, 'data': data})
                        except:
                            pass
                results = grid_results
            else:
                results = self.load_stage_results(stage_name)
            
            if results:
                # æ‰¾å‡ºæœ€ä½³çµæœ
                best_result = max(
                    results, 
                    key=lambda x: x['data'].get('metrics', {}).get('hit_rate', 0)
                )
                
                metrics = best_result['data'].get('metrics', {})
                summary.append({
                    'stage': stage_name,
                    'best_config': best_result['config_name'],
                    'hit_rate': metrics.get('hit_rate', 0),
                    'ndcg': metrics.get('ndcg', 0),
                    'rmse': metrics.get('rmse', 0),
                    'count': len(results)
                })
        
        return summary


class DatasetAnalyzer:
    """è³‡æ–™é›†çµ±è¨ˆåˆ†æå™¨"""
    
    def __init__(self, data_loader=None):
        """
        åˆå§‹åŒ–è³‡æ–™é›†åˆ†æå™¨
        
        Args:
            data_loader: DataLoader å¯¦ä¾‹ï¼Œå¦‚æœç‚º None å‰‡å‰µå»ºæ–°å¯¦ä¾‹
        """
        if data_loader is None:
            from .data_loader import DataLoader
            self.data_loader = DataLoader()
        else:
            self.data_loader = data_loader
    
    def load_ratings_sample(self, limit: int = 100000) -> Optional[pd.DataFrame]:
        """è¼‰å…¥è©•åˆ†è³‡æ–™æ¨£æœ¬"""
        try:
            # ä½¿ç”¨ DataLoader è¼‰å…¥è³‡æ–™
            _, ratings = self.data_loader.load_data(limit=limit)
            return ratings
        except Exception as e:
            print(f"âŒ è¼‰å…¥è³‡æ–™å¤±æ•—: {e}")
            return None
    
    def load_ratings_chunked(self, chunksize: int = 500000):
        """åˆ†æ‰¹è¼‰å…¥è©•åˆ†è³‡æ–™ï¼ˆç”Ÿæˆå™¨ï¼‰
        
        Args:
            chunksize: æ¯æ‰¹è®€å–çš„è³‡æ–™é‡
            
        Yields:
            pd.DataFrame: è©•åˆ†è³‡æ–™æ‰¹æ¬¡
        """
        try:
            import kagglehub
            path = kagglehub.dataset_download(self.data_loader.dataset_name)
            rating_file = f"{path}/rating.csv"
            
            # ä½¿ç”¨ chunksize åƒæ•¸åˆ†æ‰¹è®€å–
            for chunk in pd.read_csv(rating_file, usecols=['userId', 'movieId', 'rating'], 
                                     chunksize=chunksize):
                yield chunk
        except Exception as e:
            print(f"âŒ åˆ†æ‰¹è¼‰å…¥è³‡æ–™å¤±æ•—: {e}")
            return
    
    def analyze_rating_distribution(self, ratings: pd.DataFrame = None, use_full_dataset: bool = False) -> Optional[Dict[str, Any]]:
        """åˆ†æè©•åˆ†åˆ†å¸ƒ
        
        Args:
            ratings: é è¼‰å…¥çš„è©•åˆ†è³‡æ–™ï¼ˆå¦‚æœç‚º None å‰‡è‡ªå‹•è¼‰å…¥ï¼‰
            use_full_dataset: æ˜¯å¦ä½¿ç”¨å®Œæ•´è³‡æ–™é›†ï¼ˆåˆ†æ‰¹è™•ç†ï¼‰
        """
        if use_full_dataset and ratings is None:
            # ä½¿ç”¨åˆ†æ‰¹è™•ç†å®Œæ•´è³‡æ–™é›†
            print("ğŸ“Š ä½¿ç”¨åˆ†æ‰¹è™•ç†åˆ†æå®Œæ•´è³‡æ–™é›†...")
            rating_counts = {}
            total = 0
            sum_rating = 0
            sum_sq = 0
            
            for i, chunk in enumerate(self.load_ratings_chunked(), 1):
                chunk_counts = chunk['rating'].value_counts()
                for rating, count in chunk_counts.items():
                    rating_counts[rating] = rating_counts.get(rating, 0) + count
                
                total += len(chunk)
                sum_rating += chunk['rating'].sum()
                sum_sq += (chunk['rating'] ** 2).sum()
                
                if i % 10 == 0:
                    print(f"   è™•ç†æ‰¹æ¬¡ {i} (å·²è™•ç† {total:,} ç­†)...")
            
            mean = sum_rating / total
            std = np.sqrt(sum_sq / total - mean ** 2)
            
            # è¨ˆç®—ä¸­ä½æ•¸ï¼ˆå¾åˆ†å¸ƒä¼°ç®—ï¼‰
            sorted_ratings = sorted(rating_counts.items())
            cumsum = 0
            median = 3.0
            for rating, count in sorted_ratings:
                cumsum += count
                if cumsum >= total / 2:
                    median = rating
                    break
            
            stats = {
                'distribution': rating_counts,
                'mean': float(mean),
                'median': float(median),
                'std': float(std),
                'min': float(min(rating_counts.keys())),
                'max': float(max(rating_counts.keys())),
                'total_ratings': total
            }
            print(f"âœ… å®Œæˆåˆ†æ {total:,} ç­†è©•åˆ†")
            return stats
        
        # ä½¿ç”¨æ¨£æœ¬è³‡æ–™
        if ratings is None:
            ratings = self.load_ratings_sample()
        
        if ratings is None or ratings.empty:
            return None
        
        # è¨ˆç®—è©•åˆ†åˆ†å¸ƒ
        rating_counts = ratings['rating'].value_counts().sort_index()
        
        stats = {
            'distribution': rating_counts.to_dict(),
            'mean': float(ratings['rating'].mean()),
            'median': float(ratings['rating'].median()),
            'std': float(ratings['rating'].std()),
            'min': float(ratings['rating'].min()),
            'max': float(ratings['rating'].max()),
            'total_ratings': len(ratings)
        }
        
        return stats
    
    def analyze_user_activity(self, ratings: pd.DataFrame = None, use_full_dataset: bool = False, top_n: int = 10000) -> Optional[Dict[str, Any]]:
        """åˆ†æä½¿ç”¨è€…æ´»èºåº¦ï¼ˆé•·å°¾æ•ˆæ‡‰ï¼‰
        
        Args:
            ratings: é è¼‰å…¥çš„è©•åˆ†è³‡æ–™
            use_full_dataset: æ˜¯å¦ä½¿ç”¨å®Œæ•´è³‡æ–™é›†
            top_n: è¿”å›å‰ N å€‹æœ€æ´»èºç”¨æˆ¶ï¼ˆç”¨æ–¼é•·å°¾åœ–ï¼‰
        """
        if use_full_dataset and ratings is None:
            print("ğŸ“Š ä½¿ç”¨åˆ†æ‰¹è™•ç†åˆ†æç”¨æˆ¶æ´»èºåº¦...")
            user_counts_dict = {}
            total = 0
            
            for i, chunk in enumerate(self.load_ratings_chunked(), 1):
                chunk_counts = chunk['userId'].value_counts()
                for user_id, count in chunk_counts.items():
                    user_counts_dict[user_id] = user_counts_dict.get(user_id, 0) + count
                
                total += len(chunk)
                if i % 10 == 0:
                    print(f"   è™•ç†æ‰¹æ¬¡ {i} (å·²è™•ç† {total:,} ç­†)...")
            
            # è½‰æ›ç‚ºæ’åºå¾Œçš„æ•¸çµ„ï¼ˆåªä¿ç•™å‰ top_nï¼‰
            user_counts = pd.Series(user_counts_dict).sort_values(ascending=False)
            user_counts_values = user_counts.head(top_n).values
            
            print(f"âœ… å®Œæˆåˆ†æ {len(user_counts):,} ä½ç”¨æˆ¶ï¼Œä¿ç•™å‰ {top_n:,} ä½ç”¨æ–¼é•·å°¾åœ–")
        else:
            if ratings is None:
                ratings = self.load_ratings_sample()
            
            if ratings is None or ratings.empty:
                return None
            
            # è¨ˆç®—æ¯å€‹ä½¿ç”¨è€…çš„è©•åˆ†æ•¸é‡
            user_counts = ratings['userId'].value_counts()
            user_counts_values = user_counts.values
        
        # çµ±ä¸€è™•ç†çµ±è¨ˆæ•¸æ“š
        if use_full_dataset:
            user_counts_series = pd.Series(sorted(user_counts_dict.values(), reverse=True))
            total_ratings = sum(user_counts_dict.values())
        else:
            user_counts_series = user_counts
            total_ratings = user_counts.sum()
        
        top_20_percent = int(len(user_counts_series) * 0.2)
        top_20_ratings = user_counts_series.iloc[:top_20_percent].sum()
        
        stats = {
            'total_users': len(user_counts_series),
            'mean_ratings_per_user': float(user_counts_series.mean()),
            'median_ratings_per_user': float(user_counts_series.median()),
            'std_ratings_per_user': float(user_counts_series.std()),
            'max_ratings': int(user_counts_series.max()),
            'min_ratings': int(user_counts_series.min()),
            'long_tail': {
                'top_20_percent_users': top_20_percent,
                'top_20_percent_ratings': int(top_20_ratings),
                'top_20_percent_ratio': float(top_20_ratings / total_ratings)
            },
            'quantiles': {
                '25%': int(user_counts_series.quantile(0.25)),
                '50%': int(user_counts_series.quantile(0.50)),
                '75%': int(user_counts_series.quantile(0.75)),
                '90%': int(user_counts_series.quantile(0.90)),
                '95%': int(user_counts_series.quantile(0.95)),
                '99%': int(user_counts_series.quantile(0.99))
            }
        }
        
        # å¢åŠ åªç”¨æ–¼ç¹ªåœ–çš„å­—æ®µï¼ˆä¸åŒ…å«åœ¨ JSON è¼¸å‡ºä¸­ï¼‰
        stats['_plot_data'] = user_counts_values  # å‰ç¼€ _ è¡¨ç¤ºå…§éƒ¨ä½¿ç”¨
        
        return stats
    
    def analyze_item_popularity(self, ratings: pd.DataFrame = None, use_full_dataset: bool = False, top_n: int = 10000) -> Optional[Dict[str, Any]]:
        """åˆ†æé›»å½±æµè¡Œåº¦ï¼ˆå†·å•Ÿå‹•å•é¡Œï¼‰
        
        Args:
            ratings: é è¼‰å…¥çš„è©•åˆ†è³‡æ–™
            use_full_dataset: æ˜¯å¦ä½¿ç”¨å®Œæ•´è³‡æ–™é›†
            top_n: è¿”å›å‰ N å€‹æœ€ç†±é–€é›»å½±ï¼ˆç”¨æ–¼é•·å°¾åœ–ï¼‰
        """
        if use_full_dataset and ratings is None:
            print("ğŸ“Š ä½¿ç”¨åˆ†æ‰¹è™•ç†åˆ†æé›»å½±æµè¡Œåº¦...")
            item_counts_dict = {}
            total = 0
            
            for i, chunk in enumerate(self.load_ratings_chunked(), 1):
                chunk_counts = chunk['movieId'].value_counts()
                for movie_id, count in chunk_counts.items():
                    item_counts_dict[movie_id] = item_counts_dict.get(movie_id, 0) + count
                
                total += len(chunk)
                if i % 10 == 0:
                    print(f"   è™•ç†æ‰¹æ¬¡ {i} (å·²è™•ç† {total:,} ç­†)...")
            
            # è½‰æ›ç‚ºæ’åºå¾Œçš„æ•¸çµ„ï¼ˆåªä¿ç•™å‰ top_nï¼‰
            item_counts = pd.Series(item_counts_dict).sort_values(ascending=False)
            item_counts_values = item_counts.head(top_n).values
            
            print(f"âœ… å®Œæˆåˆ†æ {len(item_counts):,} éƒ¨é›»å½±ï¼Œä¿ç•™å‰ {top_n:,} éƒ¨ç”¨æ–¼é•·å°¾åœ–")
        else:
            if ratings is None:
                ratings = self.load_ratings_sample()
            
            if ratings is None or ratings.empty:
                return None
            
            # è¨ˆç®—æ¯éƒ¨é›»å½±çš„è©•åˆ†æ•¸é‡
            item_counts = ratings['movieId'].value_counts()
            item_counts_values = item_counts.values
        
        # çµ±ä¸€è™•ç†çµ±è¨ˆæ•¸æ“š
        if use_full_dataset:
            item_counts_series = pd.Series(sorted(item_counts_dict.values(), reverse=True))
            cold_items_1 = sum(1 for c in item_counts_dict.values() if c == 1)
            cold_items_5 = sum(1 for c in item_counts_dict.values() if c <= 5)
            cold_items_10 = sum(1 for c in item_counts_dict.values() if c <= 10)
        else:
            item_counts_series = item_counts
            cold_items_1 = (item_counts == 1).sum()
            cold_items_5 = (item_counts <= 5).sum()
            cold_items_10 = (item_counts <= 10).sum()
        
        stats = {
            'total_items': len(item_counts_series),
            'mean_ratings_per_item': float(item_counts_series.mean()),
            'median_ratings_per_item': float(item_counts_series.median()),
            'std_ratings_per_item': float(item_counts_series.std()),
            'max_ratings': int(item_counts_series.max()),
            'min_ratings': int(item_counts_series.min()),
            'cold_start': {
                'items_with_1_rating': int(cold_items_1),
                'items_with_le_5_ratings': int(cold_items_5),
                'items_with_le_10_ratings': int(cold_items_10),
                'cold_start_ratio_5': float(cold_items_5 / len(item_counts_series)),
                'cold_start_ratio_10': float(cold_items_10 / len(item_counts_series))
            },
            'quantiles': {
                '25%': int(item_counts_series.quantile(0.25)),
                '50%': int(item_counts_series.quantile(0.50)),
                '75%': int(item_counts_series.quantile(0.75)),
                '90%': int(item_counts_series.quantile(0.90)),
                '95%': int(item_counts_series.quantile(0.95)),
                '99%': int(item_counts_series.quantile(0.99))
            }
        }
        
        # å¢åŠ åªç”¨æ–¼ç¹ªåœ–çš„å­—æ®µï¼ˆä¸åŒ…å«åœ¨ JSON è¼¸å‡ºä¸­ï¼‰
        stats['_plot_data'] = item_counts_values  # å‰ç¼€ _ è¡¨ç¤ºå…§éƒ¨ä½¿ç”¨
        
        return stats
    
    def analyze_sparsity(self, ratings: pd.DataFrame = None) -> Optional[Dict[str, Any]]:
        """åˆ†æçŸ©é™£ç¨€ç–åº¦"""
        if ratings is None:
            ratings = self.load_ratings_sample()
        
        if ratings is None or ratings.empty:
            return None
        
        n_users = ratings['userId'].nunique()
        n_items = ratings['movieId'].nunique()
        n_ratings = len(ratings)
        
        # è¨ˆç®—ç¨€ç–åº¦
        total_possible = n_users * n_items
        sparsity = 1 - (n_ratings / total_possible)
        density = n_ratings / total_possible
        
        stats = {
            'n_users': int(n_users),
            'n_items': int(n_items),
            'n_ratings': int(n_ratings),
            'total_possible_ratings': int(total_possible),
            'sparsity': float(sparsity),
            'density': float(density),
            'sparsity_percentage': float(sparsity * 100),
            'density_percentage': float(density * 100)
        }
        
        return stats
    
    def generate_full_analysis(self, sample_size: int = 100000) -> Dict[str, Any]:
        """ç”Ÿæˆå®Œæ•´çš„è³‡æ–™é›†åˆ†æå ±å‘Š"""
        print("=" * 80)
        print("ğŸ“Š è³‡æ–™é›†çµ±è¨ˆåˆ†æ")
        print("=" * 80)
        print()
        
        # è¼‰å…¥è³‡æ–™
        print(f"ğŸ“ è¼‰å…¥è³‡æ–™æ¨£æœ¬ (å‰ {sample_size:,} ç­†)...")
        try:
            ratings = self.load_ratings_sample(limit=sample_size)
        except Exception as e:
            print(f"âŒ è¼‰å…¥è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return {}
        
        if ratings is None or ratings.empty:
            print("âŒ ç„¡æ³•è¼‰å…¥è³‡æ–™")
            return {}
        
        print(f"âœ… æˆåŠŸè¼‰å…¥ {len(ratings):,} ç­†è©•åˆ†è³‡æ–™")
        print()
        
        # åŸ·è¡Œå„é …åˆ†æ
        analysis = {
            'sample_size': len(ratings),
            'rating_distribution': self.analyze_rating_distribution(ratings),
            'user_activity': self.analyze_user_activity(ratings),
            'item_popularity': self.analyze_item_popularity(ratings),
            'sparsity': self.analyze_sparsity(ratings)
        }
        
        return analysis


def print_dataset_analysis(analyzer: DatasetAnalyzer, sample_size: int = 100000):
    """æ‰“å°è³‡æ–™é›†åˆ†æå ±å‘Š"""
    analysis = analyzer.generate_full_analysis(sample_size)
    
    if not analysis:
        print("âŒ åˆ†æå¤±æ•—")
        return
    
    # è©•åˆ†åˆ†å¸ƒ
    if analysis.get('rating_distribution'):
        stats = analysis['rating_distribution']
        print("ğŸ“Š è©•åˆ†åˆ†å¸ƒçµ±è¨ˆ")
        print("-" * 80)
        print(f"å¹³å‡è©•åˆ†: {stats['mean']:.3f}")
        print(f"ä¸­ä½æ•¸: {stats['median']:.3f}")
        print(f"æ¨™æº–å·®: {stats['std']:.3f}")
        print(f"ç¯„åœ: {stats['min']:.1f} - {stats['max']:.1f}")
        print(f"\nå„è©•åˆ†æ•¸é‡:")
        for rating, count in sorted(stats['distribution'].items()):
            percentage = count / stats['total_ratings'] * 100
            print(f"  {rating:.1f} æ˜Ÿ: {count:,} ({percentage:.2f}%)")
        print()
    
    # ä½¿ç”¨è€…æ´»èºåº¦
    if analysis.get('user_activity'):
        stats = analysis['user_activity']
        print("ğŸ‘¥ ä½¿ç”¨è€…æ´»èºåº¦åˆ†æ")
        print("-" * 80)
        print(f"ç¸½ä½¿ç”¨è€…æ•¸: {stats['total_users']:,}")
        print(f"å¹³å‡æ¯äººè©•åˆ†æ•¸: {stats['mean_ratings_per_user']:.1f}")
        print(f"ä¸­ä½æ•¸: {stats['median_ratings_per_user']:.0f}")
        print(f"æ¨™æº–å·®: {stats['std']:.1f}")
        print(f"æœ€å¤šè©•åˆ†: {stats['max_ratings']:,}")
        print(f"æœ€å°‘è©•åˆ†: {stats['min_ratings']:,}")
        print(f"\né•·å°¾æ•ˆæ‡‰:")
        print(f"  å‰ 20% æ´»èºç”¨æˆ¶è²¢ç» {stats['top_20_percent_contribution']*100:.1f}% çš„è©•åˆ†")
        print(f"\næ´»èºåº¦åˆ†ä½æ•¸:")
        for p, v in stats['percentiles'].items():
            print(f"  {p}: {v:.0f} è©•åˆ†")
        print()
    
    # é›»å½±æµè¡Œåº¦
    if analysis.get('item_popularity'):
        stats = analysis['item_popularity']
        print("ğŸ¬ é›»å½±æµè¡Œåº¦åˆ†æ")
        print("-" * 80)
        print(f"ç¸½é›»å½±æ•¸: {stats['total_items']:,}")
        print(f"å¹³å‡æ¯éƒ¨é›»å½±è©•åˆ†æ•¸: {stats['mean_ratings_per_item']:.1f}")
        print(f"ä¸­ä½æ•¸: {stats['median_ratings_per_item']:.0f}")
        print(f"æ¨™æº–å·®: {stats['std']:.1f}")
        print(f"æœ€å¤šè©•åˆ†: {stats['max_ratings']:,}")
        print(f"æœ€å°‘è©•åˆ†: {stats['min_ratings']:,}")
        print(f"\nå†·å•Ÿå‹•å•é¡Œ:")
        for threshold, data in stats['cold_items'].items():
            print(f"  è©•åˆ† {threshold}: {data['count']:,} éƒ¨ ({data['percentage']:.2f}%)")
        print(f"\né•·å°¾æ•ˆæ‡‰:")
        print(f"  å‰ 20% ç†±é–€é›»å½±è²¢ç» {stats['top_20_percent_contribution']*100:.1f}% çš„è©•åˆ†")
        print(f"\næµè¡Œåº¦åˆ†ä½æ•¸:")
        for p, v in stats['percentiles'].items():
            print(f"  {p}: {v:.0f} è©•åˆ†")
        print()
    
    # ç¨€ç–åº¦
    if analysis.get('sparsity'):
        stats = analysis['sparsity']
        print("ğŸ“ˆ çŸ©é™£ç¨€ç–åº¦åˆ†æ")
        print("-" * 80)
        print(f"ä½¿ç”¨è€…æ•¸: {stats['n_users']:,}")
        print(f"é›»å½±æ•¸: {stats['n_items']:,}")
        print(f"è©•åˆ†æ•¸: {stats['n_ratings']:,}")
        print(f"å¯èƒ½çš„è©•åˆ†ç¸½æ•¸: {stats['total_possible_ratings']:,}")
        print(f"ç¨€ç–åº¦: {stats['sparsity_percentage']:.2f}%")
        print(f"å¯†åº¦: {stats['density_percentage']:.4f}%")
        print()
    
    print("=" * 80)
    print()


def print_progress_report(analyzer: ExperimentAnalyzer):
    """æ‰“å°é€²åº¦å ±å‘Š"""
    progress = analyzer.check_progress()
    
    print("=" * 80)
    print("ğŸ“Š é‡æ§‹è¨ˆç•«åŸ·è¡Œé€²åº¦")
    print("=" * 80)
    print()
    
    for stage_name, stage_data in progress['stages'].items():
        status = "âœ…" if stage_data['rate'] == 100 else "â³"
        print(f"{status} {stage_name} éšæ®µ: {stage_data['count']}/{stage_data['total']} ({stage_data['rate']:.0f}%)")
        
        if stage_data['missing']:
            print(f"   ç¼ºå°‘: {', '.join(stage_data['missing'][:5])}")
            if len(stage_data['missing']) > 5:
                print(f"   ... é‚„æœ‰ {len(stage_data['missing']) - 5} å€‹")
        print()
    
    print("=" * 80)
    total = progress['total_configs']
    completed = progress['total_completed']
    print(f"ğŸ“ˆ ç¸½é«”é€²åº¦: {completed}/{total} ({completed/total*100:.1f}%)")
    print("=" * 80)
    print()
    
    # åˆ†æç‹€æ…‹
    print("ğŸ¯ åˆ†æç‹€æ…‹:")
    svd_status = "âœ… å·²å®Œæˆ" if progress['analysis_status']['svd'] else "â³ æœªå®Œæˆ"
    knn_status = "âœ… å·²å®Œæˆ" if progress['analysis_status']['knn'] else "â³ æœªå®Œæˆ"
    print(f"  SVD åˆ†æ: {svd_status}")
    print(f"  KNN åˆ†æ: {knn_status}")
    print()


def print_svd_analysis(analyzer: ExperimentAnalyzer):
    """æ‰“å° SVD åˆ†æçµæœ"""
    result = analyzer.analyze_svd()
    
    if not result:
        print("âŒ æœªæ‰¾åˆ° SVD éšæ®µçµæœ")
        return
    
    print("=" * 76)
    print("ğŸ“Š SVD éšæ®µçµæœåˆ†æ")
    print("=" * 76)
    print()
    print(f"ğŸ“Š æ‰¾åˆ° {result['count']} å€‹ SVD é…ç½®çµæœ")
    print()
    print("ğŸ† Top 10 SVD é…ç½®ï¼ˆæŒ‰ Hit Rate@10ï¼‰ï¼š")
    print()
    print(f"{'æ’å':<4} {'é…ç½®':<12} {'ç¶­åº¦':<6} {'Hit Rate':<10} {'NDCG':<10} {'RMSE':<10} {'æ™‚é–“(s)':<10}")
    print("-" * 76)
    
    for i, r in enumerate(result['results'][:10], 1):
        print(f"{i:<4} {r['config_name']:<12} {r['n_components']:<6} "
              f"{r['hit_rate']:<10.4f} {r['ndcg']:<10.4f} "
              f"{r['rmse']:<10.4f} {r['total_time']:<10.2f}")
    
    if result['best']:
        best = result['best']
        print()
        print(f"âœ¨ æœ€ä½³ SVD é…ç½®ï¼š{best['config_name']}")
        print(f"   ç¶­åº¦ï¼š{best['n_components']}")
        print(f"   Hit Rate@10ï¼š{best['hit_rate']:.4f}")
        print(f"   NDCG@10ï¼š{best['ndcg']:.4f}")
        print(f"   RMSEï¼š{best['rmse']:.4f}")
        print(f"   åŸ·è¡Œæ™‚é–“ï¼š{best['total_time']:.2f} ç§’")
        print(f"   è¨˜æ†¶é«”å³°å€¼ï¼š{best['memory_mb']:.2f} MB")
        print()
        print("ğŸ’¾ å·²ä¿å­˜æœ€ä½³é…ç½®è‡³: log/best_svd.json")
    
    print("=" * 76)
    print()


def print_knn_analysis(analyzer: ExperimentAnalyzer):
    """æ‰“å° KNN åˆ†æçµæœ"""
    result = analyzer.analyze_knn()
    
    if not result:
        print("âŒ æœªæ‰¾åˆ° KNN éšæ®µçµæœ")
        return
    
    print("=" * 72)
    print("ğŸ“Š KNN éšæ®µçµæœåˆ†æ")
    print("=" * 72)
    print()
    print(f"ğŸ“Š æ‰¾åˆ° {result['count']} å€‹ KNN é…ç½®çµæœ")
    print()
    print("ğŸ† KNN é…ç½®æ’åï¼ˆæŒ‰ Hit Rate@10ï¼‰ï¼š")
    print()
    print(f"{'æ’å':<4} {'é…ç½®':<12} {'Kå€¼':<6} {'Hit Rate':<10} {'NDCG':<10} {'RMSE':<10} {'æ™‚é–“(s)':<10}")
    print("-" * 72)
    
    for i, r in enumerate(result['results'], 1):
        print(f"{i:<4} {r['config_name']:<12} {r['k_neighbors']:<6} "
              f"{r['hit_rate']:<10.4f} {r['ndcg']:<10.4f} "
              f"{r['rmse']:<10.4f} {r['total_time']:<10.2f}")
    
    if result['best']:
        best = result['best']
        print()
        print(f"âœ¨ æœ€ä½³ KNN é…ç½®ï¼š{best['config_name']}")
        print(f"   K å€¼ï¼š{best['k_neighbors']}")
        print(f"   Hit Rate@10ï¼š{best['hit_rate']:.4f}")
        print(f"   NDCG@10ï¼š{best['ndcg']:.4f}")
        print(f"   RMSEï¼š{best['rmse']:.4f}")
        print(f"   åŸ·è¡Œæ™‚é–“ï¼š{best['total_time']:.2f} ç§’")
        print()
        print("ğŸ’¾ å·²ä¿å­˜æœ€ä½³é…ç½®è‡³: log/best_knn.json")
    
    print("=" * 72)
    print()


# å‘½ä»¤è¡Œæ¥å£
if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        command = sys.argv[1]
        
        if command == 'progress':
            analyzer = ExperimentAnalyzer()
            print_progress_report(analyzer)
        elif command == 'svd':
            analyzer = ExperimentAnalyzer()
            print_svd_analysis(analyzer)
        elif command == 'knn':
            analyzer = ExperimentAnalyzer()
            print_knn_analysis(analyzer)
        elif command == 'dataset':
            dataset_analyzer = DatasetAnalyzer()
            print_dataset_analysis(dataset_analyzer)
        elif command == 'all':
            # å¯¦é©—çµæœåˆ†æ
            analyzer = ExperimentAnalyzer()
            print_progress_report(analyzer)
            print()
            print_svd_analysis(analyzer)
            print()
            print_knn_analysis(analyzer)
            print()
            # è³‡æ–™é›†åˆ†æ
            dataset_analyzer = DatasetAnalyzer()
            print_dataset_analysis(dataset_analyzer)
        else:
            print("ç”¨æ³•: python -m movie_recommendation.analysis [progress|svd|knn|dataset|all]")
    else:
        # é»˜èªé¡¯ç¤ºé€²åº¦
        analyzer = ExperimentAnalyzer()
        print_progress_report(analyzer)
