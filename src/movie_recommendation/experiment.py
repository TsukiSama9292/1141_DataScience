"""
Experiment orchestration module.
"""

import time
import numpy as np
from typing import Optional, Dict
import logging
import inspect
from pathlib import Path
from .data_loader import DataLoader
from .feature_engineering import FeatureEngineer
from .models import KNNRecommender
from .evaluation import Evaluator
from .utils import TimeTracker, setup_logging, log_metrics
from movie_recommendation.hybrid_engine import GenomeHybridModel

logger = logging.getLogger(__name__)


class ExperimentConfig:
    """Configuration for a recommendation experiment."""
    
    def __init__(
        self,
        data_limit: Optional[int] = None,
        min_item_ratings: int = 0,
        use_timestamp: bool = False,
        use_item_bias: bool = False,
        use_svd: bool = False,
        n_components: int = 50,
        use_time_decay: bool = False,
        half_life_days: int = 500,
        use_tfidf: bool = False,
        k_neighbors: int = 20,
        amplification_factor: float = 1.0,
        n_samples: int = 500,
        top_n: int = 10,
        random_state: int = 42,

        use_genome_hybrid: bool = False,
        genome_alpha: float = 0.2,
        cold_start_threshold: int = 70
    ):
        """
        Initialize experiment configuration.
        
        Args:
            data_limit: Limit on number of ratings (None for full dataset)
            min_item_ratings: Minimum number of ratings per item (0 = no filtering)
            use_timestamp: Whether to load timestamp data
            use_item_bias: Whether to calculate item bias
            use_svd: Whether to apply SVD dimensionality reduction
            n_components: Number of SVD components
            use_time_decay: Whether to apply time decay weighting
            half_life_days: Half-life for time decay
            use_tfidf: Whether to apply TF-IDF weighting
            k_neighbors: Number of nearest neighbors
            amplification_factor: Similarity amplification factor
            n_samples: Number of samples for evaluation
            top_n: Number of recommendations to generate
            random_state: Random seed for reproducibility
        """
        self.data_limit = data_limit
        self.min_item_ratings = min_item_ratings
        self.use_timestamp = use_timestamp
        self.use_item_bias = use_item_bias
        self.use_svd = use_svd
        self.n_components = n_components
        self.use_time_decay = use_time_decay
        self.half_life_days = half_life_days
        self.use_tfidf = use_tfidf
        self.k_neighbors = k_neighbors
        self.amplification_factor = amplification_factor
        self.n_samples = n_samples
        self.top_n = top_n
        self.random_state = random_state


class Experiment:
    """Run a complete recommendation experiment."""
    
    def __init__(self, config: ExperimentConfig, config_name: Optional[str] = None):
        """
        Initialize experiment.
        
        Args:
            config: Experiment configuration
            config_name: Optional name for logging (auto-detected if None)
        """
        self.config = config
        
        # Auto-detect config name from calling script filename
        if config_name is None:
            frame = inspect.currentframe()
            if frame and frame.f_back and frame.f_back.f_back:
                caller_file = frame.f_back.f_back.f_code.co_filename
                config_name = Path(caller_file).stem
            else:
                config_name = "experiment"
        
        self.config_name = config_name
        self.logger = setup_logging(config_name, log_dir="log")
        self.tracker = TimeTracker()
        self.time_records = {}
        
        self.data_loader = DataLoader()
        self.feature_engineer = FeatureEngineer()
        self.recommender = KNNRecommender(
            n_neighbors=config.k_neighbors,
            metric='cosine',
            algorithm='brute',
            n_jobs=-1
        )
        self.evaluator = Evaluator()
        
        self.train_matrix = None
        self.user_features = None
        self.item_means = None
        self.global_mean = 3.5
        
    def _log_time(self, stage_name: str, start_time: float):
        """Log stage execution time."""
        elapsed = time.time() - start_time
        self.time_records[stage_name] = elapsed
        self.tracker.log_stage(stage_name, elapsed)
    
    def run(self) -> Dict[str, float]:
        """
        Run complete experiment pipeline.
        
        Returns:
            Dictionary of evaluation metrics
        """
        self.logger.info(f"é–‹å§‹é…ç½®: {self.config_name}")
        
        self.tracker.start()
        
        # Load data
        stage_start = time.time()
        if self.config.use_timestamp:
            movies, ratings = self.data_loader.load_with_timestamp(self.config.data_limit)
        else:
            movies, ratings = self.data_loader.load_data(self.config.data_limit, self.config.min_item_ratings)
        self.tracker.sample_memory()
        self._log_time("è¼‰å…¥è³‡æ–™", stage_start)
        
        # Create mappings
        stage_start = time.time()
        ratings, user_map, movie_map = self.data_loader.create_user_item_mapping(ratings)
        n_users = len(user_map)
        n_items = len(movie_map)
        self.tracker.sample_memory()
        self._log_time("å»ºç«‹æ˜ å°„", stage_start)
        
        # Calculate item bias if needed
        if self.config.use_item_bias:
            stage_start = time.time()
            self.item_means, self.global_mean = self.data_loader.calculate_item_bias(ratings)
            self.tracker.sample_memory()
            self._log_time("è¨ˆç®— Item Bias", stage_start)
        
        # Build sparse matrix
        stage_start = time.time()
        
        if self.config.use_time_decay:
            decayed_ratings, original_ratings = self.feature_engineer.apply_time_decay(
                ratings, self.config.half_life_days
            )
            # Build two matrices
            ratings_decay = ratings.copy()
            ratings_decay['rating'] = decayed_ratings
            train_matrix_decay = self.feature_engineer.build_sparse_matrix(
                ratings_decay, n_users, n_items
            )
            ratings_orig = ratings.copy()
            ratings_orig['rating'] = original_ratings
            self.train_matrix = self.feature_engineer.build_sparse_matrix(
                ratings_orig, n_users, n_items
            )
            # Use decayed for KNN, original for scoring
            matrix_for_knn = train_matrix_decay
            
        elif self.config.use_tfidf:
            tfidf_ratings, original_ratings = self.feature_engineer.apply_tfidf_weighting(
                ratings, n_users
            )
            # Build two matrices
            ratings_tfidf = ratings.copy()
            ratings_tfidf['rating'] = tfidf_ratings
            train_matrix_tfidf = self.feature_engineer.build_sparse_matrix(
                ratings_tfidf, n_users, n_items
            )
            ratings_orig = ratings.copy()
            ratings_orig['rating'] = original_ratings
            self.train_matrix = self.feature_engineer.build_sparse_matrix(
                ratings_orig, n_users, n_items
            )
            # Use TF-IDF for KNN, original for scoring
            matrix_for_knn = train_matrix_tfidf
            
        else:
            self.train_matrix = self.feature_engineer.build_sparse_matrix(
                ratings, n_users, n_items
            )
            matrix_for_knn = self.train_matrix
        
        self.tracker.sample_memory()
        self._log_time("å»ºç«‹ç¨€ç–çŸ©é™£", stage_start)
        
        # Apply SVD if needed
        if self.config.use_svd:
            stage_start = time.time()
            self.user_features = self.feature_engineer.apply_svd(
                matrix_for_knn, self.config.n_components, self.config.random_state
            )
            self.tracker.sample_memory()
            self._log_time("SVD é™ç¶­", stage_start)
        else:
            # Keep user features as sparse matrix to avoid large dense allocations
            # Many sklearn routines accept CSR sparse matrices for distance computations.
            self.user_features = matrix_for_knn
        
        # Train KNN
        stage_start = time.time()
        self.recommender.fit(self.user_features)
        self.tracker.sample_memory()
        self._log_time("è¨“ç·´ KNN", stage_start)
        
        # Evaluation
        stage_start = time.time()
        np.random.seed(self.config.random_state)
        
        # å‹•æ…‹èª¿æ•´æ¨£æœ¬æ•¸ï¼Œç¢ºä¿ä¸è¶…éå¯ç”¨ç”¨æˆ¶æ•¸
        actual_samples = min(self.config.n_samples, n_users)
        if actual_samples < self.config.n_samples:
            logger.warning(f"âš ï¸  å¯ç”¨ç”¨æˆ¶æ•¸ ({n_users}) å°‘æ–¼é…ç½®æ¨£æœ¬æ•¸ ({self.config.n_samples})ï¼Œèª¿æ•´ç‚º {actual_samples}")
        
        test_users = np.random.choice(n_users, size=actual_samples, replace=False)
        
        # Override predict_rating if amplification is used
        if self.config.amplification_factor != 1.0:
            original_predict = self.recommender.predict_rating
            def amplified_predict(user_idx, movie_idx, train_matrix, user_features,
                                item_means=None, global_mean=3.5, amplification_factor=1.0):
                return original_predict(
                    user_idx, movie_idx, train_matrix, user_features,
                    item_means, global_mean, self.config.amplification_factor
                )
            self.recommender.predict_rating = amplified_predict
        
        if getattr(self.config, 'use_genome_hybrid', False):
            self.logger.info("ğŸ§¬ [Hybrid] æ­£åœ¨å•Ÿå‹• Genome å†·å•Ÿå‹•å„ªåŒ–æ¨¡çµ„...")
            
            # 1. åˆå§‹åŒ–ä½ çš„å¼•æ“
            # ç¢ºä¿è·¯å¾‘æ­£ç¢ºï¼Œæ ¹æ“šä½ çš„å°ˆæ¡ˆçµæ§‹èª¿æ•´
            hybrid_engine = GenomeHybridModel(genome_scores_path='data/raw/genome-scores.csv')
            
            # 2. æº–å‚™ ID è½‰æ›è¡¨ (éå¸¸é‡è¦ï¼å› ç‚º Recommender ç”¨çš„æ˜¯ 0,1,2 ç´¢å¼•ï¼Œä½† CSV ç”¨çš„æ˜¯çœŸå¯¦ ID)
            # user_map: {Real_ID: Index} -> è½‰æˆ -> {Index: Real_ID}
            idx_to_user_id = {v: k for k, v in user_map.items()}
            idx_to_movie_id = {v: k for k, v in movie_map.items()}
            
            # 3. å»ºç«‹ä½¿ç”¨è€…è§€å½±æ­·å² (ç”¨æ–¼è¨ˆç®—ç›¸ä¼¼åº¦)
            # ratings æ˜¯ DataFrameï¼ŒåŒ…å« [userId, movieId, rating]
            self.logger.info("   æ­£åœ¨æ§‹å»ºæ­·å²è§€å½±å¿«å–...")
            user_history_map = ratings.groupby('userId')['movieId'].apply(list).to_dict()
            
            # 4. è®€å–è¶…åƒæ•¸
            alpha_base = getattr(self.config, 'genome_alpha', 0.2)
            cold_thresh = getattr(self.config, 'cold_start_threshold', 5)
            
            # 5. ã€æ ¸å¿ƒé­”æ³•ã€‘å·æ›æ¦‚å¿µï¼šæ””æˆªä¸¦è¦†å¯« predict_rating æ–¹æ³•
            # å…ˆæŠŠåŸæœ¬çš„é æ¸¬å‡½æ•¸å­˜èµ·ä¾† (å¯èƒ½æ˜¯ KNN é æ¸¬ï¼Œä¹Ÿå¯èƒ½æ˜¯å·²ç¶“è¢« amplification æ”¹éçš„é æ¸¬)
            base_predict_func = self.recommender.predict_rating
            
            def hybrid_predict_wrapper(user_idx, movie_idx, train_matrix, user_features,
                                     item_means=None, global_mean=3.5, *args, **kwargs):
                
                # A. å–å¾—åŸæœ¬åˆ†æ•¸ (SVD/KNN)
                try:
                    base_score = base_predict_func(
                        user_idx, movie_idx, train_matrix, user_features,
                        item_means, global_mean, *args, **kwargs
                    )
                except:
                    base_score = global_mean

                # B. è½‰æ› Index -> Real ID
                real_uid = idx_to_user_id.get(user_idx)
                real_mid = idx_to_movie_id.get(movie_idx)
                
                # C. è¨ˆç®— Genome å…§å®¹åˆ†æ•¸ (ä½ çš„è²¢ç»)
                # å¦‚æœæ‰¾ä¸åˆ° ID (æ–°é›»å½±)ï¼Œå›å‚³ 0
                if real_uid is None or real_mid is None:
                    return base_score

                history = user_history_map.get(real_uid, [])
                content_sim = hybrid_engine.get_content_score(real_mid, history)
                
                # å°‡ç›¸ä¼¼åº¦ (0~1) æ˜ å°„åˆ°è©•åˆ† (3.0 ~ 5.0)
                # é€™è£¡å‡è¨­ç›¸ä¼¼åº¦é«˜ä»£è¡¨æœƒçµ¦é«˜åˆ†
                content_score = 3.0 + (content_sim * 2.0)
                
                # D. å‹•æ…‹æ¬Šé‡ (å†·å•Ÿå‹•é‚è¼¯)
                if len(history) < cold_thresh:
                    current_alpha = getattr(self.config, 'genome_alpha')
                else:
                    current_alpha = alpha_base # è€æ‰‹ï¼šå”åŒç‚ºä¸»
                
                # E. èåˆ
                final_score = (1 - current_alpha) * base_score + current_alpha * content_score
                
                # F. ç¢ºä¿åˆ†æ•¸åˆç†
                return max(0.5, min(5.0, final_score))

            # 6. æŠŠé€™å€‹æ··åˆå‡½æ•¸ï¼Œã€Œæ›è¼‰ã€å›å»çµ¦ recommender
            self.recommender.predict_rating = hybrid_predict_wrapper
            self.logger.info("   âœ… å·²æˆåŠŸæ³¨å…¥æ··åˆé æ¸¬é‚è¼¯ (Hybrid Logic Injected)")

        metrics = self.evaluator.evaluate_leave_one_out(
            self.recommender,
            self.train_matrix,
            self.user_features,
            test_users,
            top_n=self.config.top_n,
            item_means=self.item_means,
            global_mean=self.global_mean
        )
        self.tracker.sample_memory()
        self._log_time("è©•ä¼°", stage_start)
        
        # Log results
        peak_memory = self.tracker.get_peak_mb()
        
        # æº–å‚™é…ç½®å­—å…¸
        config_dict = {
            'data_limit': self.config.data_limit,
            'min_item_ratings': self.config.min_item_ratings,
            'use_timestamp': self.config.use_timestamp,
            'use_item_bias': self.config.use_item_bias,
            'use_svd': self.config.use_svd,
            'n_components': self.config.n_components,
            'use_time_decay': self.config.use_time_decay,
            'half_life_days': self.config.half_life_days,
            'use_tfidf': self.config.use_tfidf,
            'k_neighbors': self.config.k_neighbors,
            'amplification_factor': self.config.amplification_factor,
            'top_n': self.config.top_n,
            'random_state': self.config.random_state
        }
        
        log_metrics(
            self.logger,
            metrics,
            metrics['n_samples'],
            self.time_records,
            peak_memory,
            config=config_dict
        )
        
        self.logger.info(f"é…ç½®å®Œæˆ: {self.config_name}")
        
        return metrics
