#!/usr/bin/env python3
"""
ç¶²æ ¼æœå°‹å·¥å…· - è‡ªå‹•åŒ–è¶…åƒæ•¸å„ªåŒ–
æ”¯æ´å…¨é¢çš„åƒæ•¸çµ„åˆæœå°‹ä¸¦ç”Ÿæˆå®Œæ•´å ±å‘Š
"""

import sys
import json
import time
import argparse
import itertools
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime

# æ·»åŠ  src åˆ°è·¯å¾‘
sys.path.insert(0, str(Path(__file__).parent.parent / 'src'))

from movie_recommendation.experiment import Experiment, ExperimentConfig
from movie_recommendation.analysis import ExperimentAnalyzer
from movie_recommendation.report_generator import ReportGenerator


class GridSearch:
    """ç¶²æ ¼æœå°‹å¼•æ“"""
    
    def __init__(self, output_dir: str = "grid_search_results"):
        """
        åˆå§‹åŒ–ç¶²æ ¼æœå°‹
        
        Args:
            output_dir: çµæœè¼¸å‡ºç›®éŒ„
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.results = []
        self.best_config = None
        self.best_score = -float('inf')
        
    def define_search_space(
        self,
        n_components: Optional[List[int]] = None,
        k_neighbors: Optional[List[int]] = None,
        min_item_ratings: Optional[List[int]] = None,
        use_svd: Optional[List[bool]] = None,
        use_item_bias: Optional[List[bool]] = None,
        use_time_decay: Optional[List[bool]] = None,
        use_tfidf: Optional[List[bool]] = None,
        n_samples: int = 500,
        top_n: int = 10,
        random_state: int = 42
    ) -> List[Dict[str, Any]]:
        """
        å®šç¾©æœå°‹ç©ºé–“
        
        Args:
            n_components: SVD ç¶­åº¦åˆ—è¡¨
            k_neighbors: KNN é„°å±…æ•¸åˆ—è¡¨
            min_item_ratings: æœ€å°è©•åˆ†æ•¸åˆ—è¡¨
            use_svd: æ˜¯å¦ä½¿ç”¨ SVD
            use_item_bias: æ˜¯å¦ä½¿ç”¨ Item Bias
            use_time_decay: æ˜¯å¦ä½¿ç”¨æ™‚é–“è¡°æ¸›
            use_tfidf: æ˜¯å¦ä½¿ç”¨ TF-IDF
            n_samples: è©•ä¼°æ¨£æœ¬æ•¸
            top_n: æ¨è–¦æ•¸é‡
            random_state: éš¨æ©Ÿç¨®å­
            
        Returns:
            åƒæ•¸çµ„åˆåˆ—è¡¨
        """
        # é»˜èªæœå°‹ç©ºé–“ï¼šSVD ä½¿ç”¨ 2^n (n=1..10)ï¼ŒKNN ä½¿ç”¨ 5*n (n=1..10)
        if n_components is None:
            n_components = [2**n for n in range(1, 11)]  # [2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]
        if k_neighbors is None:
            k_neighbors = [5*n for n in range(1, 11)]    # [5, 10, 15, 20, 25, 30, 35, 40, 45, 50]
        if min_item_ratings is None:
            min_item_ratings = [0]
        if use_svd is None:
            use_svd = [True]
        if use_item_bias is None:
            use_item_bias = [False]
        if use_time_decay is None:
            use_time_decay = [False]
        if use_tfidf is None:
            use_tfidf = [False]
        
        # ç”Ÿæˆæ‰€æœ‰çµ„åˆ
        param_grid = {
            'n_components': n_components,
            'k_neighbors': k_neighbors,
            'min_item_ratings': min_item_ratings,
            'use_svd': use_svd,
            'use_item_bias': use_item_bias,
            'use_time_decay': use_time_decay,
            'use_tfidf': use_tfidf,
        }
        
        # ç”Ÿæˆç¬›å¡çˆ¾ç©
        keys = list(param_grid.keys())
        values = list(param_grid.values())
        combinations = list(itertools.product(*values))
        
        configs = []
        for combo in combinations:
            config = dict(zip(keys, combo))
            # æ·»åŠ å›ºå®šåƒæ•¸
            config['data_limit'] = None
            config['n_samples'] = n_samples
            config['top_n'] = top_n
            config['random_state'] = random_state
            config['use_timestamp'] = False
            
            # å¦‚æœä¸ä½¿ç”¨ SVDï¼Œå‰‡ä¸éœ€è¦ n_components
            if not config['use_svd']:
                config['n_components'] = None
            
            configs.append(config)
        
        return configs
    
    def run_search(
        self,
        configs: List[Dict[str, Any]],
        metric: str = 'hit_rate',
        save_all: bool = True
    ) -> Dict[str, Any]:
        """
        åŸ·è¡Œç¶²æ ¼æœå°‹
        
        Args:
            configs: é…ç½®åˆ—è¡¨
            metric: å„ªåŒ–ç›®æ¨™æŒ‡æ¨™
            save_all: æ˜¯å¦ä¿å­˜æ‰€æœ‰çµæœ
            
        Returns:
            æœå°‹çµæœæ‘˜è¦
        """
        total_configs = len(configs)
        print("=" * 80)
        print("ğŸ” ç¶²æ ¼æœå°‹é–‹å§‹")
        print("=" * 80)
        print(f"ç¸½é…ç½®æ•¸: {total_configs}")
        print(f"å„ªåŒ–æŒ‡æ¨™: {metric}")
        print(f"é è¨ˆæ™‚é–“: ~{total_configs * 2 / 60:.1f} åˆ†é˜ (å‡è¨­æ¯å€‹é…ç½® 2 ç§’)")
        print("=" * 80)
        print()
        
        start_time = time.time()
        
        for idx, config_dict in enumerate(configs, 1):
            config_name = f"GRID_{idx:04d}"
            
            print(f"[{idx}/{total_configs}] åŸ·è¡Œ {config_name}...")
            
            try:
                # å‰µå»ºé…ç½®å°è±¡
                config = ExperimentConfig(**config_dict)
                
                # åŸ·è¡Œå¯¦é©—
                experiment = Experiment(config, config_name=config_name)
                metrics = experiment.run()
                
                # è¨˜éŒ„çµæœ
                result = {
                    'config_name': config_name,
                    'config': config_dict,
                    'metrics': metrics,
                    'score': metrics[metric]
                }
                
                self.results.append(result)
                
                # æ›´æ–°æœ€ä½³é…ç½®
                if result['score'] > self.best_score:
                    self.best_score = result['score']
                    self.best_config = result
                    print(f"  âœ¨ æ–°çš„æœ€ä½³é…ç½®! {metric} = {self.best_score:.4f}")
                
                # ä¿å­˜çµæœ
                if save_all:
                    result_file = self.output_dir / f"{config_name}.json"
                    with open(result_file, 'w', encoding='utf-8') as f:
                        json.dump(result, f, indent=2, ensure_ascii=False)
                
                # é€²åº¦å ±å‘Š
                elapsed = time.time() - start_time
                avg_time = elapsed / idx
                remaining = (total_configs - idx) * avg_time
                print(f"  {metric} = {result['score']:.4f} | "
                      f"å·²è€—æ™‚: {elapsed/60:.1f}åˆ† | "
                      f"é è¨ˆå‰©é¤˜: {remaining/60:.1f}åˆ†")
                
            except Exception as e:
                print(f"  âŒ éŒ¯èª¤: {e}")
                continue
        
        total_time = time.time() - start_time
        
        print()
        print("=" * 80)
        print("âœ… ç¶²æ ¼æœå°‹å®Œæˆ")
        print("=" * 80)
        print(f"ç¸½åŸ·è¡Œæ™‚é–“: {total_time/60:.1f} åˆ†é˜")
        print(f"æˆåŠŸé…ç½®æ•¸: {len(self.results)}/{total_configs}")
        print(f"æœ€ä½³ {metric}: {self.best_score:.4f}")
        print("=" * 80)
        
        # ç”Ÿæˆæ‘˜è¦
        summary = {
            'search_type': 'grid_search',
            'total_configs': total_configs,
            'successful_configs': len(self.results),
            'metric': metric,
            'best_score': self.best_score,
            'best_config': self.best_config,
            'total_time_seconds': total_time,
            'timestamp': datetime.now().isoformat()
        }
        
        # ä¿å­˜æ‘˜è¦
        summary_file = self.output_dir / 'grid_search_summary.json'
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        return summary
    
    def generate_report(self, metric: str = 'hit_rate'):
        """
        ç”Ÿæˆå®Œæ•´å ±å‘Š
        
        Args:
            metric: ä¸»è¦æŒ‡æ¨™
        """
        print("\n" + "=" * 80)
        print("ğŸ“Š ç”Ÿæˆå®Œæ•´å ±å‘Š")
        print("=" * 80)
        
        if not self.results:
            print("âŒ æ²’æœ‰çµæœå¯ä¾›åˆ†æ")
            return
        
        # æ’åºçµæœ
        sorted_results = sorted(self.results, key=lambda x: x['score'], reverse=True)
        
        # ç”Ÿæˆ Markdown å ±å‘Š
        report_file = self.output_dir / 'grid_search_report.md'
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# ç¶²æ ¼æœå°‹å®Œæ•´å ±å‘Š\n\n")
            f.write(f"ç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # æ‘˜è¦
            f.write("## æœå°‹æ‘˜è¦\n\n")
            f.write(f"- **ç¸½é…ç½®æ•¸**: {len(self.results)}\n")
            f.write(f"- **å„ªåŒ–æŒ‡æ¨™**: {metric}\n")
            f.write(f"- **æœ€ä½³å¾—åˆ†**: {self.best_score:.4f}\n\n")
            
            # Top 10 é…ç½®
            f.write("## Top 10 é…ç½®\n\n")
            f.write("| æ’å | é…ç½® | Hit Rate | NDCG | RMSE | SVD | K | å‚™è¨» |\n")
            f.write("|------|------|----------|------|------|-----|---|------|\n")
            
            for rank, result in enumerate(sorted_results[:10], 1):
                config = result['config']
                metrics = result['metrics']
                
                svd_str = f"{config.get('n_components', 'N/A')}" if config.get('use_svd') else "ç„¡"
                k_str = str(config.get('k_neighbors', 'N/A'))
                
                notes = []
                if config.get('use_item_bias'):
                    notes.append("Bias")
                if config.get('use_time_decay'):
                    notes.append("TimeDecay")
                if config.get('use_tfidf'):
                    notes.append("TF-IDF")
                note_str = ", ".join(notes) if notes else "-"
                
                f.write(f"| {rank} | {result['config_name']} | "
                       f"{metrics['hit_rate']:.4f} | "
                       f"{metrics['ndcg']:.4f} | "
                       f"{metrics['rmse']:.4f} | "
                       f"{svd_str} | {k_str} | {note_str} |\n")
            
            f.write("\n")
            
            # æœ€ä½³é…ç½®è©³æƒ…
            f.write("## æœ€ä½³é…ç½®è©³æƒ…\n\n")
            f.write("```json\n")
            f.write(json.dumps(self.best_config, indent=2, ensure_ascii=False))
            f.write("\n```\n\n")
            
            # åƒæ•¸åˆ†æ
            f.write("## åƒæ•¸å½±éŸ¿åˆ†æ\n\n")
            
            # SVD ç¶­åº¦åˆ†æ
            if any(r['config'].get('use_svd') for r in self.results):
                f.write("### SVD ç¶­åº¦å½±éŸ¿\n\n")
                svd_analysis = self._analyze_parameter('n_components', metric)
                f.write("| SVD ç¶­åº¦ | å¹³å‡ Hit Rate | é…ç½®æ•¸ |\n")
                f.write("|----------|---------------|--------|\n")
                for value, avg_score, count in sorted(svd_analysis, key=lambda x: x[1], reverse=True):
                    if value is not None:
                        f.write(f"| {value} | {avg_score:.4f} | {count} |\n")
                f.write("\n")
            
            # K å€¼åˆ†æ
            f.write("### KNN é„°å±…æ•¸å½±éŸ¿\n\n")
            k_analysis = self._analyze_parameter('k_neighbors', metric)
            f.write("| K å€¼ | å¹³å‡ Hit Rate | é…ç½®æ•¸ |\n")
            f.write("|------|---------------|--------|\n")
            for value, avg_score, count in sorted(k_analysis, key=lambda x: x[1], reverse=True):
                f.write(f"| {value} | {avg_score:.4f} | {count} |\n")
            f.write("\n")
            
            # æ‰€æœ‰çµæœ
            f.write("## æ‰€æœ‰é…ç½®çµæœ\n\n")
            f.write("| é…ç½® | Hit Rate | NDCG | RMSE | é…ç½®è©³æƒ… |\n")
            f.write("|------|----------|------|------|----------|\n")
            
            for result in sorted_results:
                config = result['config']
                metrics = result['metrics']
                
                config_str = f"SVD={config.get('n_components', 'N/A') if config.get('use_svd') else 'ç„¡'}, K={config.get('k_neighbors')}"
                
                f.write(f"| {result['config_name']} | "
                       f"{metrics['hit_rate']:.4f} | "
                       f"{metrics['ndcg']:.4f} | "
                       f"{metrics['rmse']:.4f} | "
                       f"{config_str} |\n")
        
        print(f"âœ… å ±å‘Šå·²ç”Ÿæˆ: {report_file}")
        
        # ç”Ÿæˆå¯è¦–åŒ–
        self._generate_visualizations(metric)
    
    def _analyze_parameter(self, param_name: str, metric: str) -> List[tuple]:
        """
        åˆ†æå–®å€‹åƒæ•¸çš„å½±éŸ¿
        
        Args:
            param_name: åƒæ•¸åç¨±
            metric: è©•ä¼°æŒ‡æ¨™
            
        Returns:
            (åƒæ•¸å€¼, å¹³å‡å¾—åˆ†, é…ç½®æ•¸) åˆ—è¡¨
        """
        param_scores = {}
        
        for result in self.results:
            value = result['config'].get(param_name)
            score = result['metrics'][metric]
            
            if value not in param_scores:
                param_scores[value] = []
            param_scores[value].append(score)
        
        analysis = []
        for value, scores in param_scores.items():
            avg_score = sum(scores) / len(scores)
            count = len(scores)
            analysis.append((value, avg_score, count))
        
        return analysis
    
    def _generate_visualizations(self, metric: str):
        """ç”Ÿæˆå¯è¦–åŒ–åœ–è¡¨"""
        try:
            import matplotlib.pyplot as plt
            import numpy as np
            
            # SVD ç¶­åº¦ vs æ€§èƒ½
            svd_results = {}
            for result in self.results:
                if result['config'].get('use_svd'):
                    n_comp = result['config'].get('n_components')
                    score = result['metrics'][metric]
                    if n_comp not in svd_results:
                        svd_results[n_comp] = []
                    svd_results[n_comp].append(score)
            
            if svd_results:
                fig, ax = plt.subplots(figsize=(10, 6))
                dimensions = sorted(svd_results.keys())
                avg_scores = [np.mean(svd_results[d]) for d in dimensions]
                std_scores = [np.std(svd_results[d]) for d in dimensions]
                
                ax.plot(dimensions, avg_scores, marker='o', linewidth=2, markersize=8)
                ax.fill_between(dimensions, 
                               [a - s for a, s in zip(avg_scores, std_scores)],
                               [a + s for a, s in zip(avg_scores, std_scores)],
                               alpha=0.3)
                ax.set_xlabel('SVD ç¶­åº¦', fontsize=12)
                ax.set_ylabel(f'{metric.upper()}', fontsize=12)
                ax.set_title('SVD ç¶­åº¦å½±éŸ¿åˆ†æ', fontsize=14, fontweight='bold')
                ax.grid(True, alpha=0.3)
                
                plt.tight_layout()
                plt.savefig(self.output_dir / 'svd_analysis.png', dpi=300, bbox_inches='tight')
                plt.close()
                print("âœ… å·²ç”Ÿæˆ SVD åˆ†æåœ–")
            
            # K å€¼ vs æ€§èƒ½
            k_results = {}
            for result in self.results:
                k = result['config'].get('k_neighbors')
                score = result['metrics'][metric]
                if k not in k_results:
                    k_results[k] = []
                k_results[k].append(score)
            
            if k_results:
                fig, ax = plt.subplots(figsize=(10, 6))
                k_values = sorted(k_results.keys())
                avg_scores = [np.mean(k_results[k]) for k in k_values]
                std_scores = [np.std(k_results[k]) for k in k_values]
                
                ax.plot(k_values, avg_scores, marker='s', linewidth=2, markersize=8)
                ax.fill_between(k_values,
                               [a - s for a, s in zip(avg_scores, std_scores)],
                               [a + s for a, s in zip(avg_scores, std_scores)],
                               alpha=0.3)
                ax.set_xlabel('KNN é„°å±…æ•¸ (K)', fontsize=12)
                ax.set_ylabel(f'{metric.upper()}', fontsize=12)
                ax.set_title('KNN é„°å±…æ•¸å½±éŸ¿åˆ†æ', fontsize=14, fontweight='bold')
                ax.grid(True, alpha=0.3)
                
                plt.tight_layout()
                plt.savefig(self.output_dir / 'knn_analysis.png', dpi=300, bbox_inches='tight')
                plt.close()
                print("âœ… å·²ç”Ÿæˆ KNN åˆ†æåœ–")
                
        except ImportError:
            print("âš ï¸  ç„¡æ³•ç”Ÿæˆå¯è¦–åŒ– (éœ€è¦ matplotlib)")


def main():
    parser = argparse.ArgumentParser(description='ç¶²æ ¼æœå°‹å·¥å…·')
    parser.add_argument('--preset', type=str, choices=['quick', 'standard', 'full'],
                       default='standard', help='é è¨­æœå°‹æ¨¡å¼')
    parser.add_argument('--output', type=str, default='grid_search_results',
                       help='è¼¸å‡ºç›®éŒ„')
    parser.add_argument('--metric', type=str, default='hit_rate',
                       help='å„ªåŒ–æŒ‡æ¨™')
    parser.add_argument('--samples', type=int, default=500,
                       help='è©•ä¼°æ¨£æœ¬æ•¸')
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–æœå°‹å™¨
    searcher = GridSearch(output_dir=args.output)
    
    # æ ¹æ“šé è¨­æ¨¡å¼å®šç¾©æœå°‹ç©ºé–“
    if args.preset == 'quick':
        # å¿«é€Ÿæœå°‹ï¼šå°‘é‡é…ç½® (5Ã—5=25å€‹å¯¦é©—)
        configs = searcher.define_search_space(
            n_components=[8, 32, 128, 512, 1024],  # 2^3, 2^5, 2^7, 2^9, 2^10
            k_neighbors=[5, 15, 25, 35, 50],        # 5*1, 5*3, 5*5, 5*7, 5*10
            n_samples=args.samples
        )
    elif args.preset == 'standard':
        # æ¨™æº–æœå°‹ï¼šå¹³è¡¡é…ç½® (10Ã—10=100å€‹å¯¦é©—)
        configs = searcher.define_search_space(
            n_components=[2**n for n in range(1, 11)],  # [2, 4, 8, ..., 1024]
            k_neighbors=[5*n for n in range(1, 11)],     # [5, 10, 15, ..., 50]
            n_samples=args.samples
        )
    else:  # full
        # å®Œæ•´æœå°‹ï¼šæ›´å¯†é›†çš„é…ç½® (15Ã—15=225å€‹å¯¦é©—)
        configs = searcher.define_search_space(
            n_components=[2**n for n in range(1, 11)] + [384, 512, 768, 1024, 1536],  # æ“´å±•ç¯„åœ
            k_neighbors=[5*n for n in range(1, 16)],  # [5, 10, 15, ..., 75]
            n_samples=args.samples
        )
    
    # åŸ·è¡Œæœå°‹
    summary = searcher.run_search(configs, metric=args.metric)
    
    # ç”Ÿæˆå ±å‘Š
    searcher.generate_report(metric=args.metric)
    
    print("\n" + "=" * 80)
    print("ğŸ‰ ç¶²æ ¼æœå°‹å®Œæˆï¼")
    print("=" * 80)
    print(f"æœ€ä½³é…ç½®: {summary['best_config']['config_name']}")
    print(f"æœ€ä½³ {args.metric}: {summary['best_score']:.4f}")
    print(f"çµæœä¿å­˜æ–¼: {args.output}/")
    print("=" * 80)


if __name__ == "__main__":
    main()
